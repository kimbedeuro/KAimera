{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29eaaa1f-bfbd-4748-ac55-33de62aa8d08",
   "metadata": {},
   "source": [
    "## Installation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7dc19b2-7042-470e-94d5-068e924adb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- --------------------\n",
      "absl-py                      1.4.0\n",
      "alembic                      1.13.1\n",
      "anyio                        4.2.0\n",
      "argon2-cffi                  23.1.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.3.0\n",
      "asttokens                    2.4.1\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.4\n",
      "attrs                        23.2.0\n",
      "Babel                        2.14.0\n",
      "backcall                     0.2.0\n",
      "beautifulsoup4               4.12.3\n",
      "bleach                       6.1.0\n",
      "Brotli                       1.0.9\n",
      "cachetools                   5.3.1\n",
      "certifi                      2023.11.17\n",
      "cffi                         1.15.0\n",
      "chardet                      5.2.0\n",
      "charset-normalizer           2.0.4\n",
      "cmaes                        0.10.0\n",
      "colorlog                     6.8.0\n",
      "comm                         0.2.1\n",
      "contourpy                    1.1.0\n",
      "cryptography                 41.0.3\n",
      "cycler                       0.11.0\n",
      "debugpy                      1.8.1\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "eTaPR                        22.6.1\n",
      "exceptiongroup               1.2.0\n",
      "executing                    2.0.1\n",
      "fastjsonschema               2.19.1\n",
      "filelock                     3.13.1\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.42.0\n",
      "fqdn                         1.5.1\n",
      "fsspec                       2023.12.2\n",
      "gast                         0.4.0\n",
      "gmpy2                        2.1.2\n",
      "google-auth                  2.22.0\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "greenlet                     3.0.3\n",
      "grpcio                       1.57.0\n",
      "h11                          0.14.0\n",
      "h5py                         3.9.0\n",
      "httpcore                     1.0.3\n",
      "httpx                        0.26.0\n",
      "idna                         3.4\n",
      "importlib-metadata           6.8.0\n",
      "importlib-resources          6.0.1\n",
      "ipykernel                    6.29.2\n",
      "ipython                      8.12.3\n",
      "ipywidgets                   8.1.2\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.1\n",
      "Jinja2                       3.1.2\n",
      "joblib                       1.3.2\n",
      "json5                        0.9.14\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.21.1\n",
      "jsonschema-specifications    2023.12.1\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               8.6.0\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.7.1\n",
      "jupyter-events               0.9.0\n",
      "jupyter-lsp                  2.2.2\n",
      "jupyter_server               2.12.5\n",
      "jupyter_server_terminals     0.5.2\n",
      "jupyterlab                   4.1.1\n",
      "jupyterlab_pygments          0.3.0\n",
      "jupyterlab_server            2.25.3\n",
      "jupyterlab_widgets           3.0.10\n",
      "keras                        2.11.0\n",
      "kiwisolver                   1.4.4\n",
      "libclang                     16.0.6\n",
      "Mako                         1.3.0\n",
      "Markdown                     3.4.4\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.7.2\n",
      "matplotlib-inline            0.1.6\n",
      "mistune                      3.0.2\n",
      "mkl-fft                      1.3.1\n",
      "mkl-random                   1.2.2\n",
      "mkl-service                  2.4.0\n",
      "mpmath                       1.3.0\n",
      "nbclient                     0.9.0\n",
      "nbconvert                    7.16.0\n",
      "nbformat                     5.9.2\n",
      "nest-asyncio                 1.6.0\n",
      "networkx                     3.1\n",
      "notebook                     7.1.0\n",
      "notebook_shim                0.2.4\n",
      "numpy                        1.24.3\n",
      "oauthlib                     3.2.2\n",
      "opencv-python                4.8.0.76\n",
      "opt-einsum                   3.3.0\n",
      "optuna                       3.3.0\n",
      "overrides                    7.7.0\n",
      "packaging                    23.1\n",
      "pandas                       2.0.3\n",
      "pandocfilters                1.5.1\n",
      "parso                        0.8.3\n",
      "pexpect                      4.9.0\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       10.0.0\n",
      "pip                          23.2.1\n",
      "pkgutil_resolve_name         1.3.10\n",
      "platformdirs                 4.2.0\n",
      "prometheus_client            0.20.0\n",
      "prompt-toolkit               3.0.43\n",
      "protobuf                     3.19.6\n",
      "psutil                       5.9.8\n",
      "ptyprocess                   0.7.0\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.0\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "Pygments                     2.17.2\n",
      "pyOpenSSL                    23.2.0\n",
      "pyparsing                    3.0.9\n",
      "PySocks                      1.7.1\n",
      "python-dateutil              2.8.2\n",
      "python-json-logger           2.0.7\n",
      "pytz                         2023.3\n",
      "PyYAML                       6.0.1\n",
      "pyzmq                        25.1.2\n",
      "qtconsole                    5.5.1\n",
      "QtPy                         2.4.1\n",
      "referencing                  0.33.0\n",
      "requests                     2.31.0\n",
      "requests-oauthlib            1.3.1\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rpds-py                      0.18.0\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.3.0\n",
      "scipy                        1.10.1\n",
      "Send2Trash                   1.8.2\n",
      "setuptools                   68.0.0\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "soupsieve                    2.5\n",
      "SQLAlchemy                   2.0.25\n",
      "stack-data                   0.6.3\n",
      "sympy                        1.12\n",
      "tensorboard                  2.11.2\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.11.0\n",
      "tensorflow-estimator         2.11.0\n",
      "tensorflow-io-gcs-filesystem 0.33.0\n",
      "termcolor                    2.3.0\n",
      "terminado                    0.18.0\n",
      "thop                         0.1.1.post2209072238\n",
      "threadpoolctl                3.2.0\n",
      "tinycss2                     1.2.1\n",
      "tomli                        2.0.1\n",
      "torch                        2.1.2\n",
      "torch-summary                1.4.5\n",
      "torchaudio                   2.1.2\n",
      "torchvision                  0.16.2\n",
      "tornado                      6.4\n",
      "tqdm                         4.66.1\n",
      "traitlets                    5.14.1\n",
      "types-python-dateutil        2.8.19.20240106\n",
      "typing_extensions            4.7.1\n",
      "tzdata                       2023.3\n",
      "uri-template                 1.3.0\n",
      "urllib3                      1.26.16\n",
      "wcwidth                      0.2.13\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.7.0\n",
      "Werkzeug                     2.3.7\n",
      "wheel                        0.38.4\n",
      "widgetsnbextension           4.0.10\n",
      "wrapt                        1.15.0\n",
      "zipp                         3.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6038fc0-aba9-47a6-8fc6-5e1d0167aac0",
   "metadata": {},
   "source": [
    "## Import package and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8677efbc-1f5d-4856-9600-f56426e890c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 21:27:10.912491: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 21:27:11.827817: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /data/kiho/cuda-11.0/extras/CUPTI/lib64:/data/kiho/cuda-11.0/lib64:/data/kiho/cuda-11.0/extras/CUPTI/lib64:/data/kiho/cuda-11.0/lib64::/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64:/usr/local/cuda-11.7/lib64\n",
      "2024-02-18 21:27:11.827931: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /data/kiho/cuda-11.0/extras/CUPTI/lib64:/data/kiho/cuda-11.0/lib64:/data/kiho/cuda-11.0/extras/CUPTI/lib64:/data/kiho/cuda-11.0/lib64::/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64:/usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64:/usr/local/cuda-11.7/lib64\n",
      "2024-02-18 21:27:11.827942: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_5290/1044595389.py:34: experimental_run_functions_eagerly (from tensorflow.python.eager.polymorphic_function.quarantine) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "def find_best_f1(pred, label, min_thd, max_thd, n_bins):\n",
    "    f1_scores = []\n",
    "    term = (max_thd - min_thd)/(n_bins-1)\n",
    "    if isinstance(pred, torch.Tensor):\n",
    "        pred = pred.cpu().numpy()\n",
    "    for i in range(n_bins):\n",
    "        pred_labels = put_labels(pred, min_thd + i*term)\n",
    "        f1_scores.append(f1_score(label, pred_labels))\n",
    "    \n",
    "    max_id = f1_scores.index(max(f1_scores))\n",
    "\n",
    "    if f1_scores[max(max_id-1, 0)] == f1_scores[max_id] == f1_scores[min(max_id+1, n_bins-1)]:\n",
    "        return min_thd + max_id*term, f1_scores[max_id]\n",
    "    else:\n",
    "        return find_best_f1(pred, label, max(min_thd + max_id*term - term/2, min_thd), min(min_thd + max_id*term + term/2, max_thd), n_bins)\n",
    "\n",
    "def put_labels(distance, threshold):\n",
    "    distance = np.array(distance)\n",
    "    threshold = np.array(threshold)  \n",
    "    xs = np.zeros_like(distance)\n",
    "    xs[distance > threshold] = 1\n",
    "    return xs\n",
    "\n",
    "def calc_p2p(predict, actual):\n",
    "    tp = np.sum(predict * actual)\n",
    "    tn = np.sum((1-predict) * (1-actual))\n",
    "    fp = np.sum(predict * (1-actual))\n",
    "    fn = np.sum((1-predict) * actual)\n",
    "    \n",
    "    precision = tp / (tp + fp + 0.000001)\n",
    "    recall = tp / (tp + fn + 0.000001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.000001)\n",
    "    return f1, precision, recall, tp, tn, fp, fn\n",
    "\n",
    "def get_trad_f1(score, label):\n",
    "    score = np.asarray(score)\n",
    "    maxx = float(score.max())\n",
    "    minn = float(score.min())\n",
    "    \n",
    "    label = np.asarray(label)\n",
    "    actual = label > 0.1\n",
    "    \n",
    "    grain = 1000\n",
    "    max_f1 = 0.0\n",
    "    max_f1_thres = 0.0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    for i in range(grain):\n",
    "        thres = (maxx-minn)/grain * i + minn\n",
    "        predict = score > thres\n",
    "        f1, precision, recall, tp, tn, fp, fn = calc_p2p(predict, actual)\n",
    "        if f1 > max_f1:\n",
    "            max_f1 = f1\n",
    "            max_f1_thres = thres\n",
    "            p = precision\n",
    "            r = recall\n",
    "            \n",
    "    return max_f1, max_f1_thres, p, r\n",
    "\n",
    "def get_test_f1(score, label,thres):\n",
    "    score = np.asarray(score)\n",
    "    maxx = float(score.max())\n",
    "    minn = float(score.min())\n",
    "    \n",
    "    label = np.asarray(label)\n",
    "    actual = label > 0.1\n",
    "    \n",
    "    grain = 1000\n",
    "    max_f1 = 0.0\n",
    "    max_f1_thres = 0.0\n",
    "    p = 0\n",
    "    r = 0\n",
    "       \n",
    "    predict = score > thres\n",
    "    f1, precision, recall, tp, tn, fp, fn = calc_p2p(predict, actual)\n",
    "    max_f1 = f1\n",
    "    max_f1_thres = thres\n",
    "    p = precision\n",
    "    r = recall\n",
    "            \n",
    "    \n",
    "    return max_f1, max_f1_thres, p, r\n",
    "\n",
    "\n",
    "    \n",
    "def get_best_f1(score, label):\n",
    "    score = np.asarray(score)\n",
    "    maxx = float(score.max())\n",
    "    minn = float(score.min())\n",
    "    \n",
    "    grain = 10\n",
    "    max_f1 = 0.0\n",
    "    max_f1_thres = 0.0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    for i in range(grain):\n",
    "        thres = (maxx-minn)/grain * i + minn\n",
    "        # thres = i / grain\n",
    "        predict, actual = point_adjust(score, label, thres=thres)\n",
    "        f1, precision, recall, tp, tn, fp, fn = calc_p2p(predict, actual)\n",
    "        if f1 > max_f1:\n",
    "            max_f1 = f1\n",
    "            max_f1_thres = thres\n",
    "            p = precision\n",
    "            r = recall\n",
    "            \n",
    "    return max_f1, max_f1_thres, p, r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505666c-7ee4-4aab-a972-9d832e020dba",
   "metadata": {},
   "source": [
    "## Dataset load & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c8b1f4-e176-47a3-9e0f-49ea6b543a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5290/2942711443.py:4: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Training_SWaT_RAW = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT/SWaT_Dataset_Normal_v1.csv\")\n"
     ]
    }
   ],
   "source": [
    "#=========================================================== Data load======================================================================================\n",
    "\n",
    "\n",
    "Training_SWaT_RAW = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT/SWaT_Dataset_Normal_v1.csv\")\n",
    "\n",
    "TEST_SWaT_RAW = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT/SWaT_Dataset_Attack_v0.csv\") \n",
    "\n",
    "\n",
    "C_TEST_SWaT_RAW=TEST_SWaT_RAW.drop([' Timestamp','label'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "MTS_cad_SWaT_1 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/1_MTS_CAD_prediction_score.csv\")\n",
    "MTAD_gat_2 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/2_mtad_gat_prediction_score.csv\")\n",
    "GANF_3 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/3_ganf_prediction_score.csv\")\n",
    "ANOMALY_transformer_4 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/4_anomaly_transformer_prediction_score.csv\")\n",
    "RANSynCoder_5 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/5_RANS_prediction_score.csv\")\n",
    "Autoencoder_6 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/6_autoencoder_prediction_score.csv\")\n",
    "USAD_7 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/7_usad_prediction_score.csv\")\n",
    "GDN_8 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/8_gdn_prediction_score.csv\")\n",
    "LSTM_9 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/9_lstm_prediction_score.csv\")\n",
    "MSCRED_10 =pd.read_csv(\"/home/bedro/000_KD/2024_dataset/SWaT_prediction_value/10_mscred_prediction_score.csv\")\n",
    "\n",
    "\n",
    "list_SWaT_model=[MTS_cad_SWaT_1['score'],MTAD_gat_2['score'],GANF_3['score'],ANOMALY_transformer_4['score'],RANSynCoder_5['score'],Autoencoder_6['score'],USAD_7['score'],GDN_8['score'], LSTM_9['score'],MSCRED_10['score']] ###########\n",
    "\n",
    "SWaT_anomaly_score_concate = pd.concat((list_SWaT_model[0], list_SWaT_model[1], list_SWaT_model[2], list_SWaT_model[3], list_SWaT_model[4], list_SWaT_model[5], list_SWaT_model[6], list_SWaT_model[7], list_SWaT_model[8], list_SWaT_model[9]), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "SWaT_label=TEST_SWaT_RAW['label']\n",
    "\n",
    "\n",
    "\n",
    "#=========================================================== Data split======================================================================================\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(SWaT_anomaly_score_concate, SWaT_label, test_size=0.95,  random_state=1234)\n",
    "\n",
    "C_X_train, C_X_test, C_y_train, C_y_test = train_test_split(C_TEST_SWaT_RAW, SWaT_label, test_size = 0.95, random_state=1234)\n",
    "\n",
    "\n",
    "SWaT_feature_score_concate = pd.concat((C_X_train,X_train), axis = 1)\n",
    "\n",
    "SWaT_feature_score_concate_valid = pd.concat((C_X_train,X_train), axis = 1)\n",
    "\n",
    "SWaT_feature_score_concate_test = pd.concat((C_X_test,X_test), axis = 1)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(SWaT_feature_score_concate.values), torch.FloatTensor(y_train.values))\n",
    "\n",
    "valid_dataset = TensorDataset(torch.FloatTensor(SWaT_feature_score_concate_valid.values), torch.FloatTensor(y_train.values))\n",
    "\n",
    "test_dataset = TensorDataset(torch.FloatTensor(SWaT_feature_score_concate_test.values), torch.FloatTensor(C_y_test.values))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773c6e0-4e83-42c0-a725-52108dff259d",
   "metadata": {},
   "source": [
    "## Class of Meta-learner model (teacher model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34856366-0030-4df4-85d7-4a0b03e6813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim2, hidden_dim3, hidden_dim4, activation_fn_name):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.activation_fn1 = self._get_activation_fn(activation_fn_name)\n",
    "        #self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim2)\n",
    "        #self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.activation_fn2 = self._get_activation_fn(activation_fn_name)\n",
    "        #self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        #self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.activation_fn3 = self._get_activation_fn(activation_fn_name)\n",
    "        #self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        #self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.activation_fn4 = self._get_activation_fn(activation_fn_name)\n",
    "        #self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc5= nn.Linear(hidden_dim4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = self.activation_fn1(x)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.bn2(x)\n",
    "        x = self.activation_fn2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        #x = self.bn3(x)\n",
    "        x = self.activation_fn3(x)\n",
    "        #x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        #x = self.bn3(x)\n",
    "        x = self.activation_fn4(x)\n",
    "        #x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def _get_activation_fn(self, name):\n",
    "        \"\"\"Return an activation function given its name.\"\"\"\n",
    "        if name == \"ReLU\":\n",
    "            return nn.ReLU()\n",
    "        elif name == \"LeakyReLU\":\n",
    "            return nn.LeakyReLU()\n",
    "        elif name == \"Tanh\":\n",
    "            return nn.Tanh()\n",
    "        elif name == \"Sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde2f0f-5e9d-4f52-9482-3960472884df",
   "metadata": {},
   "source": [
    "## Evaluation of Teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384495f6-76ec-41bd-9a11-7439e3dfc685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params:  {'hidden_dim': 149, 'hidden_dim2': 154, 'hidden_dim3': 141, 'hidden_dim4': 77, 'lr': 3.1884778106480725e-05, 'activation_fn': 'Tanh'}\n",
      "teacher_model.summary():  NeuralNet(\n",
      "  (fc1): Linear(in_features=61, out_features=149, bias=True)\n",
      "  (activation_fn1): Tanh()\n",
      "  (fc2): Linear(in_features=149, out_features=154, bias=True)\n",
      "  (activation_fn2): Tanh()\n",
      "  (fc3): Linear(in_features=154, out_features=141, bias=True)\n",
      "  (activation_fn3): Tanh()\n",
      "  (fc4): Linear(in_features=141, out_features=77, bias=True)\n",
      "  (activation_fn4): Tanh()\n",
      "  (fc5): Linear(in_features=77, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            9,238\n",
      "├─Tanh: 1-2                              --\n",
      "├─Linear: 1-3                            23,100\n",
      "├─Tanh: 1-4                              --\n",
      "├─Linear: 1-5                            21,855\n",
      "├─Tanh: 1-6                              --\n",
      "├─Linear: 1-7                            10,934\n",
      "├─Tanh: 1-8                              --\n",
      "├─Linear: 1-9                            78\n",
      "├─Sigmoid: 1-10                          --\n",
      "=================================================================\n",
      "Total params: 65,205\n",
      "Trainable params: 65,205\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "meta-learner FLOPs: 64683.0, meta-learner Parameters: 65205.0\n",
      "Teacher model f1 score is 0.872263 and threshold is 0.997000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_load_student = joblib.load('/home/bedro/000_KD/teacher_student/best_optuna.pkl')\n",
    "\n",
    "df_teacher = teacher_load_student.trials_dataframe().drop(['number','datetime_start','datetime_complete','duration','state'], axis=1)\n",
    "\n",
    "trial_num = 1\n",
    "\n",
    "best_params = teacher_load_student.trials[trial_num].params\n",
    "\n",
    "print(\"best_params: \", best_params)\n",
    "\n",
    "\n",
    "teacher_model = NeuralNet(input_dim=SWaT_feature_score_concate.shape[1], \n",
    "                  hidden_dim=best_params[\"hidden_dim\"], \n",
    "                  hidden_dim2=best_params[\"hidden_dim2\"],\n",
    "                  hidden_dim3=best_params[\"hidden_dim3\"],\n",
    "                  hidden_dim4=best_params[\"hidden_dim4\"],\n",
    "                  activation_fn_name=best_params[\"activation_fn\"])\n",
    "    \n",
    "print(\"teacher_model.summary(): \",teacher_model)              \n",
    "                  \n",
    "teacher_model.load_state_dict(torch.load(f'/home/bedro/000_KD/teacher_student/Teacher_model_trial_{trial_num}.pth'))\n",
    "teacher_model.eval()\n",
    "\n",
    "\n",
    "input_data = torch.randn(1, 61)\n",
    "summary(teacher_model, input_size=input_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_tensor_meta = torch.tensor(SWaT_feature_score_concate.iloc[0].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "meta_flops, meta_params = profile(teacher_model, inputs=(input_tensor_meta,))\n",
    "\n",
    "print(f\"meta-learner FLOPs: {meta_flops}, meta-learner Parameters: {meta_params}\")\n",
    "\n",
    "\n",
    "y_pred_values_valid=[]\n",
    "y_true_valid=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in valid_loader:  #test_loader to valid loader\n",
    "        output_valid = teacher_model(data).squeeze()\n",
    "        y_pred_values_valid.extend(output_valid.tolist())\n",
    "        y_true_valid.extend(target.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred_values_test = []\n",
    "y_true_test = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = teacher_model(data).squeeze()\n",
    "        y_pred_values_test.extend(output.tolist())\n",
    "        batch_true_labels_test = [int(label) for label in target.tolist()]\n",
    "        y_true_test.extend(batch_true_labels_test)\n",
    "\n",
    "# Use the threshold that gave the best F1 score during training\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "best_threshold = 0\n",
    "max_f1 = 0\n",
    "for thd in thresholds:\n",
    "    y_pred = [1 if y > thd else 0 for y in y_pred_values_test]\n",
    "    f1 = f1_score(y_true_test, y_pred, zero_division=1)\n",
    "    if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        best_threshold = thd\n",
    "\n",
    "\n",
    "valid_f1,valid_treshold,_,_=get_trad_f1(y_pred_values_valid, y_true_valid)\n",
    "\n",
    "test_f1,test_treshold,p,r=get_test_f1(y_pred_values_test, y_true_test,valid_treshold)\n",
    "\n",
    "print(\"Teacher model f1 score is %f and threshold is %f\\n\" %(test_f1, test_treshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf06b24-4c45-4d63-b76f-6a68534be1ac",
   "metadata": {},
   "source": [
    "## Definition of knowledge distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38cefe2-3111-4ba8-8b7d-ae23f6380702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def knowledge_distillation_loss(y_true, student_y_pred, teacher_preds_value, alpha, temperature): #0.5 1.0\n",
    "    # Ensure that the student predictions have the same shape as the true labels\n",
    "    student_y_pred = torch.squeeze(student_y_pred)\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(student_y_pred, y_true)#F.binary_cross_entropy_with_logits(student_y_pred, y_true)\n",
    "\n",
    "    # Soften predictions and calculate distillation loss\n",
    "    teacher_soft = torch.sigmoid(teacher_preds_value / temperature)\n",
    "    student_soft = torch.sigmoid(student_y_pred / temperature)\n",
    "    kd_loss = F.mse_loss(student_soft, teacher_soft)#F.mse_loss(student_soft, teacher_soft)#F.binary_cross_entropy_with_logits(student_soft, teacher_soft)\n",
    "\n",
    "    # Combine losses\n",
    "    combined_loss = (1 - alpha) * kd_loss + alpha * ce_loss\n",
    "    return combined_loss\n",
    "\n",
    "\n",
    "def train_on_batch(model, dataset_zip, optimizer, alpha, temp):\n",
    "    total_loss = 0\n",
    "    for (teacher_pred, X_train), true_label in dataset_zip:\n",
    "        # Convert TensorFlow tensors to PyTorch tensors\n",
    "        teacher_pred = torch.from_numpy(teacher_pred.numpy()).float()\n",
    "        X_train = torch.from_numpy(X_train.numpy()).float()\n",
    "        true_label = torch.from_numpy(true_label.numpy()).float()\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()  # Clear existing gradients\n",
    "        student_y_pred = model(X_train)\n",
    "        loss = knowledge_distillation_loss(true_label, student_y_pred, teacher_pred, alpha, temp)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataset_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0400da-683a-4983-85fc-44ad1a9983e6",
   "metadata": {},
   "source": [
    "## Class of student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e80088aa-7221-4f57-ae12-c1b11b163b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(51, 149)\n",
    "        self.fc2 = torch.nn.Linear(149, 154)\n",
    "        self.fc3 = torch.nn.Linear(154, 141)\n",
    "        self.fc4 = torch.nn.Linear(141, 77)\n",
    "        self.fc5 = torch.nn.Linear(77, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return torch.sigmoid(self.fc5(x))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd0bc5-2727-43e8-bc06-15157fd07895",
   "metadata": {},
   "source": [
    "## Evaluation of student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3364eea3-9d5d-4b3a-b6d6-49ad111c8778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student parameter\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            9,238\n",
      "├─Tanh: 1-2                              --\n",
      "├─Linear: 1-3                            23,100\n",
      "├─Tanh: 1-4                              --\n",
      "├─Linear: 1-5                            21,855\n",
      "├─Tanh: 1-6                              --\n",
      "├─Linear: 1-7                            10,934\n",
      "├─Tanh: 1-8                              --\n",
      "├─Linear: 1-9                            78\n",
      "├─Sigmoid: 1-10                          --\n",
      "=================================================================\n",
      "Total params: 65,205\n",
      "Trainable params: 65,205\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 21:28:07.766718: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 21:28:10.037890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 339 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:02:00.0, compute capability: 7.0\n",
      "2024-02-18 21:28:10.038630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30183 MB memory:  -> device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:03:00.0, compute capability: 7.0\n",
      "2024-02-18 21:28:10.039207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30183 MB memory:  -> device: 2, name: Tesla V100-PCIE-32GB, pci bus id: 0000:82:00.0, compute capability: 7.0\n",
      "2024-02-18 21:28:10.039780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30183 MB memory:  -> device: 3, name: Tesla V100-PCIE-32GB, pci bus id: 0000:83:00.0, compute capability: 7.0\n",
      "Training:   0%|                                                                                                                                            | 0/30 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|████▍                                                                                                                               | 1/30 [00:01<00:35,  1.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00305538923434638\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|████████▊                                                                                                                           | 2/30 [00:02<00:33,  1.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0016736587832714247\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████████████▏                                                                                                                      | 3/30 [00:03<00:32,  1.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.001551306899459219\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█████████████████▌                                                                                                                  | 4/30 [00:04<00:32,  1.24s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0014302696898870306\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|██████████████████████                                                                                                              | 5/30 [00:05<00:29,  1.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.001341354801266706\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██████████████████████████▍                                                                                                         | 6/30 [00:07<00:27,  1.14s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0012599068149008665\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██████████████████████████████▊                                                                                                     | 7/30 [00:08<00:26,  1.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011832667897599756\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|███████████████████████████████████▏                                                                                                | 8/30 [00:09<00:24,  1.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011038043559210564\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███████████████████████████████████████▌                                                                                            | 9/30 [00:10<00:23,  1.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010291710280357206\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███████████████████████████████████████████▋                                                                                       | 10/30 [00:11<00:24,  1.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009660114727952615\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|████████████████████████████████████████████████                                                                                   | 11/30 [00:12<00:22,  1.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009146514510121051\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████████████████████████████████████████████████████▍                                                                              | 12/30 [00:13<00:20,  1.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000873642941650649\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████████████████████████████████████████████████████████▊                                                                          | 13/30 [00:15<00:19,  1.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008397652921830327\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|█████████████████████████████████████████████████████████████▏                                                                     | 14/30 [00:16<00:18,  1.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0008114779105441853\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████████████████████████████████████████████████████████████████▌                                                                 | 15/30 [00:17<00:17,  1.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007849864286736296\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████████████████████████████████████████████████████████████████████▊                                                             | 16/30 [00:18<00:16,  1.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007633460978139924\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|██████████████████████████████████████████████████████████████████████████▏                                                        | 17/30 [00:19<00:14,  1.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007437553517503788\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████████████████████████████████████████████████████████████████████████▌                                                    | 18/30 [00:20<00:13,  1.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007250980304162754\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████████████████████████████████████████████████████████████████████████████████▉                                                | 19/30 [00:21<00:12,  1.14s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0007082156220070084\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|███████████████████████████████████████████████████████████████████████████████████████▎                                           | 20/30 [00:23<00:11,  1.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006754811593261613\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████████████████████████████████████████████████████████████████████████████████████████▋                                       | 21/30 [00:24<00:10,  1.20s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006529406702627098\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|████████████████████████████████████████████████████████████████████████████████████████████████                                   | 22/30 [00:25<00:09,  1.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000640034887149971\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 23/30 [00:27<00:08,  1.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006306164975378372\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 24/30 [00:28<00:07,  1.32s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006237624324825219\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                     | 25/30 [00:29<00:06,  1.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000616787868757094\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 26/30 [00:31<00:05,  1.31s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006111081491711316\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 27/30 [00:32<00:03,  1.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006051829861775104\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 28/30 [00:33<00:02,  1.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006002363364762624\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 29/30 [00:34<00:01,  1.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005964301242322924\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:35<00:00,  1.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005910536501115938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs: 63193.0, Parameters: 63715.0\n",
      "Student model f1 score is 0.863134 and threshold is 0.136007\n",
      "\n",
      "(0.8631335230222661, 0.13600705543425284, 0.9276897506719626, 0.8069783132374558)\n"
     ]
    }
   ],
   "source": [
    "def get_trad_f1_final(score, label):\n",
    "    score = np.asarray(score)\n",
    "    maxx = float(score.max())\n",
    "    minn = float(score.min())\n",
    "    \n",
    "   \n",
    "    label = np.asarray(label)\n",
    "    actual = label > 0.1\n",
    "    \n",
    "    predict = score > valid_thresh\n",
    "    max_f1, p, r, tp, tn, fp, fn = calc_p2p(predict, actual)\n",
    "    \n",
    "    \n",
    "    max_f1_thres= valid_thresh       \n",
    "    print(\"Student model f1 score is %f and threshold is %f\\n\" %(max_f1, valid_thresh))\n",
    "    return max_f1, max_f1_thres, p, r\n",
    "\n",
    "\n",
    "y_train_pred_values_pretrain_teacher = []\n",
    "y_train_true = []\n",
    "\n",
    "# Pre-trained teacher model, we put train dataset to get a predictive output\n",
    "with torch.no_grad():\n",
    "    for data, target in train_loader:\n",
    "        output = teacher_model(data).squeeze()\n",
    "        y_train_pred_values_pretrain_teacher.extend(output.tolist())\n",
    "        batch_true_labels = [int(label) for label in target.tolist()]\n",
    "        y_train_true.extend(batch_true_labels)\n",
    "\n",
    "student_model = StudentModel()  # Create an instance of the model\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(),lr=3.1884778106480725e-05)  # Pass the model instance  0.00001\n",
    "\n",
    "print(\"student parameter\")\n",
    "\n",
    "input_data = torch.randn(1, 51)\n",
    "summary(teacher_model, input_size=input_data)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "alpha_value = 0\n",
    "temperature_value = 1\n",
    "\n",
    "dataset_12 = tf.data.Dataset.from_tensor_slices((y_train_pred_values_pretrain_teacher, C_X_train))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices(C_y_train)\n",
    "dataset_zip = tf.data.Dataset.zip((dataset_12, dataset_label)).batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Training\", unit=\"epoch\"):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    loss = train_on_batch(student_model, dataset_zip, optimizer, alpha=alpha_value, temp=temperature_value)\n",
    "    print(f'Loss: {loss}')\n",
    "\n",
    "\n",
    "student_model.eval()\n",
    "\n",
    "#####use validation set to decide threshold \n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    y_predicted_valid = student_model(torch.tensor(C_X_train.to_numpy(), dtype=torch.float32))\n",
    "    #y_predicted = student_model(torch.tensor(SWaT_feature_score_concate_test.to_numpy(), dtype=torch.float32))\n",
    "\n",
    "y_predicted_np_valid = int(y_predicted_valid.numpy()) if y_predicted_valid.requires_grad else y_predicted_valid.detach().numpy()\n",
    "\n",
    "\n",
    "predict_valid = y_predicted_np_valid.reshape(-1)\n",
    "actual_valid = C_y_train.to_numpy().reshape(-1)\n",
    "\n",
    "\n",
    "unique_values_valid_predict, counts_valid_predict = np.unique(y_predicted_np_valid, return_counts=True)\n",
    "\n",
    "\n",
    "unique_values_ground_valid_actual, counts_ground_valid_actual = np.unique(actual_valid, return_counts=True)\n",
    "\n",
    "\n",
    "valid_f1,valid_thresh,valid_p,valid_c = get_trad_f1(predict_valid,actual_valid)\n",
    "\n",
    "\n",
    "input_tensor = torch.tensor(C_X_test.iloc[[0]].to_numpy(), dtype=torch.float32)\n",
    "flops, params = profile(student_model, inputs=(input_tensor,))\n",
    "\n",
    "print(f\"FLOPs: {flops}, Parameters: {params}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### use test dataset\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    y_predicted = student_model(torch.tensor(C_X_test.to_numpy(), dtype=torch.float32))\n",
    "    #y_predicted = student_model(torch.tensor(SWaT_feature_score_concate_test.to_numpy(), dtype=torch.float32))\n",
    "\n",
    "\n",
    "\n",
    "y_predicted_np = int(y_predicted.numpy()) if y_predicted.requires_grad else y_predicted.detach().numpy()\n",
    "\n",
    "\n",
    "predict = y_predicted_np.reshape(-1)\n",
    "actual = C_y_test.to_numpy().reshape(-1)\n",
    "\n",
    "\n",
    "unique_values, counts = np.unique(y_predicted_np, return_counts=True)\n",
    "\n",
    "unique_values_ground, counts_ground = np.unique(actual, return_counts=True)\n",
    "\n",
    "\n",
    "\n",
    "print(get_trad_f1_final(predict,actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb34a6-3387-4d80-813e-bb5d793b9f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KD_jupyter",
   "language": "python",
   "name": "kd_new_2_clone_v3_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
