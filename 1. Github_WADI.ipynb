{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668b53fb-09fc-404f-848c-08dc44b8f035",
   "metadata": {},
   "source": [
    "##### [Notice]\n",
    "##### When you implement the code, please change the data path (refer in Rawcode/Github_WADI.py file).\n",
    "##### Jupyter file shows the result of running code in author PC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f96a9-d2b3-4561-a29f-e98f7041258f",
   "metadata": {},
   "source": [
    "## Import package and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed83ced-fb78-4aaa-8857-f87258ba3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from eval_utils import *\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "def find_best_f1(pred, label, min_thd, max_thd, n_bins):\n",
    "    f1_scores = []\n",
    "    term = (max_thd - min_thd)/(n_bins-1)\n",
    "    if isinstance(pred, torch.Tensor):\n",
    "        pred = pred.cpu().numpy()\n",
    "    for i in range(n_bins):\n",
    "        pred_labels = put_labels(pred, min_thd + i*term)\n",
    "        f1_scores.append(f1_score(label, pred_labels))\n",
    "    \n",
    "    max_id = f1_scores.index(max(f1_scores))\n",
    "\n",
    "    if f1_scores[max(max_id-1, 0)] == f1_scores[max_id] == f1_scores[min(max_id+1, n_bins-1)]:\n",
    "        return min_thd + max_id*term, f1_scores[max_id]\n",
    "    else:\n",
    "        return find_best_f1(pred, label, max(min_thd + max_id*term - term/2, min_thd), min(min_thd + max_id*term + term/2, max_thd), n_bins)\n",
    "\n",
    "def put_labels(distance, threshold):\n",
    "    distance = np.array(distance)\n",
    "    threshold = np.array(threshold)  \n",
    "    xs = np.zeros_like(distance)\n",
    "    xs[distance > threshold] = 1\n",
    "    return xs\n",
    "\n",
    "def calc_p2p(predict, actual):\n",
    "    tp = np.sum(predict * actual)\n",
    "    tn = np.sum((1-predict) * (1-actual))\n",
    "    fp = np.sum(predict * (1-actual))\n",
    "    fn = np.sum((1-predict) * actual)\n",
    "    \n",
    "    precision = tp / (tp + fp + 0.000001)\n",
    "    recall = tp / (tp + fn + 0.000001)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 0.000001)\n",
    "    return f1, precision, recall, tp, tn, fp, fn\n",
    "\n",
    "def get_trad_f1(score, label):\n",
    "    score = np.asarray(score)\n",
    "    maxx = float(score.max())\n",
    "    minn = float(score.min())\n",
    "    \n",
    "    label = np.asarray(label)\n",
    "    actual = label > 0.1\n",
    "    \n",
    "    grain = 1000\n",
    "    max_f1 = 0.0\n",
    "    max_f1_thres = 0.0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    for i in range(grain):\n",
    "        thres = (maxx-minn)/grain * i + minn\n",
    "        predict = score > thres\n",
    "        f1, precision, recall, tp, tn, fp, fn = calc_p2p(predict, actual)\n",
    "        if f1 > max_f1:\n",
    "            max_f1 = f1\n",
    "            max_f1_thres = thres\n",
    "            p = precision\n",
    "            r = recall\n",
    "            \n",
    "    return max_f1, max_f1_thres, p, r\n",
    "\n",
    "def get_test_f1(score, label,thres):\n",
    "    score = np.asarray(score)\n",
    "    maxx = float(score.max())\n",
    "    minn = float(score.min())\n",
    "    \n",
    "    label = np.asarray(label)\n",
    "    actual = label > 0.1\n",
    "    \n",
    "    grain = 1000\n",
    "    max_f1 = 0.0\n",
    "    max_f1_thres = 0.0\n",
    "    p = 0\n",
    "    r = 0\n",
    "       \n",
    "    predict = score > thres\n",
    "    f1, precision, recall, tp, tn, fp, fn = calc_p2p(predict, actual)\n",
    "    max_f1 = f1\n",
    "    max_f1_thres = thres\n",
    "    p = precision\n",
    "    r = recall\n",
    "            \n",
    "    \n",
    "    return max_f1, max_f1_thres, p, r\n",
    "\n",
    "\n",
    "    \n",
    "def get_best_f1(score, label):\n",
    "    score = np.asarray(score)\n",
    "    maxx = float(score.max())\n",
    "    minn = float(score.min())\n",
    "    \n",
    "    grain = 10\n",
    "    max_f1 = 0.0\n",
    "    max_f1_thres = 0.0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    for i in range(grain):\n",
    "        thres = (maxx-minn)/grain * i + minn\n",
    "        # thres = i / grain\n",
    "        predict, actual = point_adjust(score, label, thres=thres)\n",
    "        f1, precision, recall, tp, tn, fp, fn = calc_p2p(predict, actual)\n",
    "        if f1 > max_f1:\n",
    "            max_f1 = f1\n",
    "            max_f1_thres = thres\n",
    "            p = precision\n",
    "            r = recall\n",
    "            \n",
    "    return max_f1, max_f1_thres, p, r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6b679-ef65-4a16-998b-3309f2e7a77a",
   "metadata": {},
   "source": [
    "## Dataset load & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53de664c-676a-4107-8db0-f1791ab99433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================== Data load======================================================================================\n",
    "Training_WADI_RAW = pd.read_csv(\"/home/bedro/000_KD/WADI/WADI_train.csv\")\n",
    "\n",
    "TEST_WADI_RAW = pd.read_csv(\"/home/bedro/000_KD/WADI/WADI_test.csv\")\n",
    "\n",
    "\n",
    "C_TEST_WADI_RAW=TEST_WADI_RAW.drop(['attack'], axis = 1)\n",
    "\n",
    "\n",
    "MTS_cad_WADI_1 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/1_WADI_MTS_CAD_prediction_score.csv\")\n",
    "MTAD_gat_2 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/2_WADI_mtad_gat_prediction_score.csv\")\n",
    "GANF_3 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/3_WADI_ganf_prediction_score.csv\")\n",
    "ANOMALY_transformer_4 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/4_WADI_anomaly_transformer_prediction_score.csv\")\n",
    "RANSynCoder_5 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/5_WADI_RANSyn_prediction_score.csv\")\n",
    "Autoencoder_6 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/6_WADI_Autoencoder_prediction_score.csv\")\n",
    "USAD_7 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/7_WADI_USAD_prediction_score.csv\")\n",
    "GDN_8 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/8_WADI_GDN_w_prediction_scores.csv\")\n",
    "LSTM_9 = pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/9_WADI_lstm_prediction_score.csv\")\n",
    "MSCRED_10 =pd.read_csv(\"/home/bedro/000_KD/2024_dataset/WADI_prediction_value/10_WADI_mscred_prediction_score.csv\")\n",
    "\n",
    "\n",
    "list_WADI_model=[MTS_cad_WADI_1['score'],MTAD_gat_2['score'],GANF_3['score'],ANOMALY_transformer_4['score'],RANSynCoder_5['score'],Autoencoder_6['score'],USAD_7['score'],GDN_8['score'], LSTM_9['score'],MSCRED_10['score']] ###########\n",
    "\n",
    "\n",
    "WADI_anomaly_score_concate = pd.concat((list_WADI_model[0], list_WADI_model[1], list_WADI_model[2], list_WADI_model[3], list_WADI_model[4], list_WADI_model[5], list_WADI_model[6], list_WADI_model[7], list_WADI_model[8], list_WADI_model[9]), axis = 1)\n",
    "\n",
    "\n",
    "WADI_label=TEST_WADI_RAW['attack']\n",
    "\n",
    "\n",
    "#=========================================================== Data split======================================================================================\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(WADI_anomaly_score_concate, WADI_label, test_size=0.92,  random_state=1234)\n",
    "\n",
    "C_X_train, C_X_test, C_y_train, C_y_test = train_test_split(C_TEST_WADI_RAW, WADI_label, test_size = 0.92, random_state=1234)\n",
    "\n",
    "\n",
    "\n",
    "WADI_feature_score_concate = pd.concat((C_X_train,X_train), axis = 1)\n",
    "\n",
    "WADI_feature_score_concate_valid = pd.concat((C_X_train,X_train), axis = 1)\n",
    "\n",
    "WADI_feature_score_concate_test = pd.concat((C_X_test,X_test), axis = 1)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(WADI_feature_score_concate.values), torch.FloatTensor(y_train.values))##############\n",
    "\n",
    "valid_dataset = TensorDataset(torch.FloatTensor(WADI_feature_score_concate_valid.values), torch.FloatTensor(y_train.values))##############\n",
    "\n",
    "test_dataset = TensorDataset(torch.FloatTensor(WADI_feature_score_concate_test.values), torch.FloatTensor(C_y_test.values))###################\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ca885-fcab-4c9a-bdd2-9af056b877ae",
   "metadata": {},
   "source": [
    "## Class of Meta-learner model (teacher model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ffafe63-3f0c-4ce0-9a04-8026444d3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim2, hidden_dim3, hidden_dim4, activation_fn_name):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.activation_fn1 = self._get_activation_fn(activation_fn_name)\n",
    "        #self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim2)\n",
    "        #self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.activation_fn2 = self._get_activation_fn(activation_fn_name)\n",
    "        #self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        #self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.activation_fn3 = self._get_activation_fn(activation_fn_name)\n",
    "        #self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        #self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.activation_fn4 = self._get_activation_fn(activation_fn_name)\n",
    "        #self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc5= nn.Linear(hidden_dim4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        #x = self.bn1(x)\n",
    "        x = self.activation_fn1(x)\n",
    "        #x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        #x = self.bn2(x)\n",
    "        x = self.activation_fn2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        #x = self.bn3(x)\n",
    "        x = self.activation_fn3(x)\n",
    "        #x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        #x = self.bn3(x)\n",
    "        x = self.activation_fn4(x)\n",
    "        #x = self.dropout4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def _get_activation_fn(self, name):\n",
    "        \"\"\"Return an activation function given its name.\"\"\"\n",
    "        if name == \"ReLU\":\n",
    "            return nn.ReLU()\n",
    "        elif name == \"LeakyReLU\":\n",
    "            return nn.LeakyReLU()\n",
    "        elif name == \"Tanh\":\n",
    "            return nn.Tanh()\n",
    "        elif name == \"Sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1a8ae-23bf-4fba-9904-33466aeb42d8",
   "metadata": {},
   "source": [
    "## Evaluation of Teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "372766d8-d5f0-41e9-97fb-1e8d42b6c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            9,514\n",
      "├─LeakyReLU: 1-2                         --\n",
      "├─Linear: 1-3                            10,944\n",
      "├─LeakyReLU: 1-4                         --\n",
      "├─Linear: 1-5                            26,316\n",
      "├─LeakyReLU: 1-6                         --\n",
      "├─Linear: 1-7                            29,237\n",
      "├─LeakyReLU: 1-8                         --\n",
      "├─Linear: 1-9                            170\n",
      "├─Sigmoid: 1-10                          --\n",
      "=================================================================\n",
      "Total params: 76,181\n",
      "Trainable params: 76,181\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "meta-learner FLOPs: 75616.0, meta-learner Parameters: 76181.0\n",
      "Teacher model f1 score is 0.695874 and threshold is 0.996000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_load_student = joblib.load('/home/bedro/000_KD/WADI_teacher_student/best_optuna.pkl')\n",
    "\n",
    "df_teacher = teacher_load_student.trials_dataframe().drop(['number','datetime_start','datetime_complete','duration','state'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "trial_num = 4\n",
    "\n",
    "best_params = teacher_load_student.trials[trial_num].params\n",
    "\n",
    "\n",
    "teacher_model = NeuralNet(input_dim=WADI_feature_score_concate.shape[1], \n",
    "                  hidden_dim=best_params[\"hidden_dim\"], \n",
    "                  hidden_dim2=best_params[\"hidden_dim2\"],\n",
    "                  hidden_dim3=best_params[\"hidden_dim3\"],\n",
    "                  hidden_dim4=best_params[\"hidden_dim4\"],\n",
    "                  activation_fn_name=best_params[\"activation_fn\"])\n",
    "             \n",
    "                  \n",
    "teacher_model.load_state_dict(torch.load(f'/home/bedro/000_KD/WADI_teacher_student/Teacher_model_trial_{trial_num}.pth'))\n",
    "teacher_model.eval()\n",
    "\n",
    "\n",
    "input_data = torch.randn(1, 133)\n",
    "summary(teacher_model, input_size=input_data)\n",
    "\n",
    "\n",
    "input_tensor_meta = torch.tensor(WADI_feature_score_concate.iloc[0].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "meta_flops, meta_params = profile(teacher_model, inputs=(input_tensor_meta,))\n",
    "\n",
    "print(f\"meta-learner FLOPs: {meta_flops}, meta-learner Parameters: {meta_params}\")\n",
    "\n",
    "\n",
    "y_pred_values_valid=[]\n",
    "y_true_valid=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in valid_loader:  \n",
    "        output_valid = teacher_model(data).squeeze()\n",
    "        y_pred_values_valid.extend(output_valid.tolist())\n",
    "        y_true_valid.extend(target.tolist())\n",
    "\n",
    "y_pred_values_test = []\n",
    "y_true_test = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = teacher_model(data).squeeze()\n",
    "        #y_pred_values_test.extend(output.tolist())\n",
    "        y_pred_values_test.extend(output.flatten().tolist())\n",
    "        batch_true_labels_test = [int(label) for label in target.tolist()]\n",
    "        y_true_test.extend(batch_true_labels_test)\n",
    "\n",
    "\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "best_threshold = 0\n",
    "max_f1 = 0\n",
    "for thd in thresholds:\n",
    "    y_pred = [1 if y > thd else 0 for y in y_pred_values_test]\n",
    "    f1 = f1_score(y_true_test, y_pred, zero_division=1)\n",
    "    if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        best_threshold = thd\n",
    "\n",
    "\n",
    "valid_f1,valid_treshold,_,_=get_trad_f1(y_pred_values_valid, y_true_valid)\n",
    "\n",
    "test_f1,test_treshold,precision,recall=get_test_f1(y_pred_values_test, y_true_test,valid_treshold)\n",
    "\n",
    "print(\"Teacher model f1 score is %f and threshold is %f\\n\" %(test_f1, test_treshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6d463-eabb-447b-be01-ac9cb06ea5e0",
   "metadata": {},
   "source": [
    "## Definition of knowledge distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7dc4181-3085-4cdd-b03e-7b09e90c0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def knowledge_distillation_loss(y_true, student_y_pred, teacher_preds_value, alpha, temperature): #0.5 1.0\n",
    "    # Ensure that the student predictions have the same shape as the true labels\n",
    "    student_y_pred = torch.squeeze(student_y_pred)\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(student_y_pred, y_true)#F.binary_cross_entropy_with_logits(student_y_pred, y_true)\n",
    "\n",
    "    # Soften predictions and calculate distillation loss\n",
    "    teacher_soft = torch.sigmoid(teacher_preds_value / temperature)\n",
    "    student_soft = torch.sigmoid(student_y_pred / temperature)\n",
    "    kd_loss = F.mse_loss(student_soft, teacher_soft)#F.binary_cross_entropy_with_logits(student_soft, teacher_soft) ################################### MSE, CE\n",
    "\n",
    "    # Combine losses\n",
    "    combined_loss = (1 - alpha) * kd_loss + alpha * ce_loss\n",
    "    return combined_loss\n",
    "\n",
    "\n",
    "def train_on_batch(model, dataset_zip, optimizer, alpha, temp):\n",
    "    total_loss = 0\n",
    "    for (teacher_pred, X_train), true_label in dataset_zip:\n",
    "        # Convert TensorFlow tensors to PyTorch tensors\n",
    "        teacher_pred = torch.from_numpy(teacher_pred.numpy()).float()\n",
    "        X_train = torch.from_numpy(X_train.numpy()).float()\n",
    "        true_label = torch.from_numpy(true_label.numpy()).float()\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()  # Clear existing gradients\n",
    "        student_y_pred = model(X_train)\n",
    "        loss = knowledge_distillation_loss(true_label, student_y_pred, teacher_pred, alpha, temp)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataset_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da343e2-fa73-46b0-9e6d-5c39b8927780",
   "metadata": {},
   "source": [
    "## Class of student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be268cc4-e831-408c-9496-a930e395353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(123, 71)\n",
    "        self.fc2 = torch.nn.Linear(71, 152)\n",
    "        self.fc3 = torch.nn.Linear(152, 172)\n",
    "        self.fc4 = torch.nn.Linear(172, 169)\n",
    "        self.fc5 = torch.nn.Linear(169, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.tanh(self.fc4(x))\n",
    "        return torch.sigmoid(self.fc5(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093ae79-0fa5-4919-ad4f-7af764a0e861",
   "metadata": {},
   "source": [
    "## Evaluation of student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06cc079-77a5-4a99-a1ed-36f4ac2342c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student parameter\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            9,514\n",
      "├─LeakyReLU: 1-2                         --\n",
      "├─Linear: 1-3                            10,944\n",
      "├─LeakyReLU: 1-4                         --\n",
      "├─Linear: 1-5                            26,316\n",
      "├─LeakyReLU: 1-6                         --\n",
      "├─Linear: 1-7                            29,237\n",
      "├─LeakyReLU: 1-8                         --\n",
      "├─Linear: 1-9                            170\n",
      "├─Sigmoid: 1-10                          --\n",
      "=================================================================\n",
      "Total params: 76,181\n",
      "Trainable params: 76,181\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 21:22:20.949329: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-18 21:22:23.134800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30971 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:02:00.0, compute capability: 7.0\n",
      "2024-02-18 21:22:23.135516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30668 MB memory:  -> device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:03:00.0, compute capability: 7.0\n",
      "2024-02-18 21:22:23.136137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 30668 MB memory:  -> device: 2, name: Tesla V100-PCIE-32GB, pci bus id: 0000:82:00.0, compute capability: 7.0\n",
      "2024-02-18 21:22:23.136691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 30668 MB memory:  -> device: 3, name: Tesla V100-PCIE-32GB, pci bus id: 0000:83:00.0, compute capability: 7.0\n",
      "Training:   0%|                                                                                                                                            | 0/20 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|██████▌                                                                                                                             | 1/20 [00:00<00:13,  1.44epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.017042043307272e-05\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████████████▏                                                                                                                      | 2/20 [00:01<00:11,  1.51epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.264330447298301e-05\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|███████████████████▊                                                                                                                | 3/20 [00:02<00:11,  1.46epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4841169993297222e-05\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██████████████████████████▍                                                                                                         | 4/20 [00:02<00:11,  1.44epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2096368227109832e-05\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|█████████████████████████████████                                                                                                   | 5/20 [00:03<00:10,  1.49epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.478677891242118e-06\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███████████████████████████████████████▌                                                                                            | 6/20 [00:04<00:09,  1.42epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.785603597861859e-06\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|██████████████████████████████████████████████▏                                                                                     | 7/20 [00:04<00:09,  1.40epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.619445251586464e-06\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████████████████████████████████████████████████████▊                                                                               | 8/20 [00:05<00:08,  1.40epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.951898677880951e-06\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|███████████████████████████████████████████████████████████▍                                                                        | 9/20 [00:06<00:07,  1.43epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.048395915784937e-06\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████████████████████████████████████████████████████████████████▌                                                                 | 10/20 [00:06<00:06,  1.50epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.51092281636203e-06\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|████████████████████████████████████████████████████████████████████████                                                           | 11/20 [00:07<00:06,  1.29epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.881900914322302e-06\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████████████████████████████████████████████████████████████████████████████▌                                                    | 12/20 [00:08<00:06,  1.23epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.240067616105499e-05\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|█████████████████████████████████████████████████████████████████████████████████████▏                                             | 13/20 [00:09<00:05,  1.27epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.850091499008506e-06\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████████████████████████████████████████████████████████████████████████████████████████▋                                       | 14/20 [00:10<00:05,  1.14epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.237508027149384e-05\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                | 15/20 [00:11<00:04,  1.13epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.532258769474379e-06\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 16/20 [00:12<00:03,  1.10epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.705424064306158e-06\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 17/20 [00:13<00:02,  1.20epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.103950348316576e-06\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 18/20 [00:13<00:01,  1.24epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.212703138964772e-05\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 19/20 [00:14<00:00,  1.28epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0163939479678484e-05\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:15<00:00,  1.31epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2136950410880694e-05\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 74906.0, Parameters: 75471.0\n",
      "Student model f1 score is 0.689333 and threshold is 0.953983\n",
      "\n",
      "(0.6893329132029887, 0.9539827458004146, 0.738196352643243, 0.6465375778747907)\n"
     ]
    }
   ],
   "source": [
    "def get_trad_f1_final(score, label):\n",
    "    score = np.asarray(score)\n",
    "    maxx = float(score.max())\n",
    "    minn = float(score.min())\n",
    "    \n",
    "    label = np.asarray(label)\n",
    "    actual = label > 0.1\n",
    "    \n",
    "    predict = score > valid_thresh\n",
    "    max_f1, p, r, tp, tn, fp, fn = calc_p2p(predict, actual)\n",
    "    \n",
    "    \n",
    "    max_f1_thres= valid_thresh       \n",
    "    print(\"Student model f1 score is %f and threshold is %f\\n\" %(max_f1, valid_thresh))\n",
    "    return max_f1, max_f1_thres, p, r\n",
    "\n",
    "y_train_pred_values_pretrain_teacher = []\n",
    "y_train_true = []\n",
    "\n",
    "# Pre-trained teacher model, we put train dataset to get a predictive output\n",
    "with torch.no_grad():\n",
    "    for data, target in train_loader:\n",
    "        output = teacher_model(data).squeeze()\n",
    "        y_train_pred_values_pretrain_teacher.extend(output.tolist())\n",
    "        batch_true_labels = [int(label) for label in target.tolist()]\n",
    "        y_train_true.extend(batch_true_labels)\n",
    "\n",
    "student_model = StudentModel()  # Create an instance of the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(),lr=0.0001354368553070506)  # Pass the model instance  0.00001\n",
    "\n",
    "print(\"student parameter\")\n",
    "\n",
    "input_data = torch.randn(1, 123)\n",
    "summary(teacher_model, input_size=input_data)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "alpha_value = 0\n",
    "temperature_value = 10\n",
    "\n",
    "\n",
    "\n",
    "dataset_12 = tf.data.Dataset.from_tensor_slices((y_train_pred_values_pretrain_teacher, C_X_train))\n",
    "dataset_label = tf.data.Dataset.from_tensor_slices(C_y_train)\n",
    "dataset_zip = tf.data.Dataset.zip((dataset_12, dataset_label)).batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Training\", unit=\"epoch\"):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    loss = train_on_batch(student_model, dataset_zip, optimizer, alpha=alpha_value, temp=temperature_value)\n",
    "    print(f'Loss: {loss}')\n",
    "\n",
    "\n",
    "\n",
    "student_model.eval()\n",
    "\n",
    "\n",
    "#####use validation set to decide threshold \n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    y_predicted_valid = student_model(torch.tensor(C_X_train.to_numpy(), dtype=torch.float32))\n",
    "    #y_predicted = student_model(torch.tensor(SWaT_feature_score_concate_test.to_numpy(), dtype=torch.float32))\n",
    "\n",
    "\n",
    "\n",
    "y_predicted_np_valid = int(y_predicted_valid.numpy()) if y_predicted_valid.requires_grad else y_predicted_valid.detach().numpy()\n",
    "\n",
    "\n",
    "predict_valid = y_predicted_np_valid.reshape(-1)\n",
    "actual_valid = C_y_train.to_numpy().reshape(-1)\n",
    "\n",
    "\n",
    "unique_values_valid_predict, counts_valid_predict = np.unique(y_predicted_np_valid, return_counts=True)\n",
    "\n",
    "\n",
    "unique_values_ground_valid_actual, counts_ground_valid_actual = np.unique(actual_valid, return_counts=True)\n",
    "\n",
    "\n",
    "valid_f1,valid_thresh,valid_p,valid_c = get_trad_f1(predict_valid,actual_valid)\n",
    "\n",
    "\n",
    "input_tensor = torch.tensor(C_X_test.iloc[[0]].to_numpy(), dtype=torch.float32)\n",
    "flops, params = profile(student_model, inputs=(input_tensor,))\n",
    "\n",
    "print(f\"FLOPs: {flops}, Parameters: {params}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### use test dataset\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    y_predicted = student_model(torch.tensor(C_X_test.to_numpy(), dtype=torch.float32))\n",
    "    #y_predicted = student_model(torch.tensor(SWaT_feature_score_concate_test.to_numpy(), dtype=torch.float32))\n",
    "\n",
    "\n",
    "y_predicted_np = int(y_predicted.numpy()) if y_predicted.requires_grad else y_predicted.detach().numpy()\n",
    "\n",
    "\n",
    "predict = y_predicted_np.reshape(-1)\n",
    "actual = C_y_test.to_numpy().reshape(-1)\n",
    "\n",
    "\n",
    "unique_values, counts = np.unique(y_predicted_np, return_counts=True)\n",
    "\n",
    "\n",
    "unique_values_ground, counts_ground = np.unique(actual, return_counts=True)\n",
    "\n",
    "print(get_trad_f1_final(predict,actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5ca05-0070-4bc4-94a9-f0fde509a501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
